{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Costa Oeste de los Estados Unidos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Costa Oeste de EEUU incluyen a \"California, Oregón, Washington, Alaska, Arizona y Nevada\"\n",
    "En este trabajo no se tendrá en cuenta a Alaska ya que ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan librerias\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajando con Json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_contenido_carpeta(ruta_carpeta):\n",
    "    \"\"\"\n",
    "    Lee el contenido de una carpeta y devuelve una lista de diccionarios con los datos de los archivos JSON.\n",
    "\n",
    "    Args:\n",
    "        ruta_carpeta (str): Ruta de la carpeta a leer.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de diccionarios con los datos de los archivos JSON.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Verificar si la ruta es un directorio\n",
    "        if os.path.isdir(ruta_carpeta):\n",
    "            # Obtener lista de archivos y directorios en la carpeta\n",
    "            contenido = os.listdir(ruta_carpeta)\n",
    "            \n",
    "            # Lista para almacenar datos de todos los archivos JSON en la carpeta\n",
    "            df = []\n",
    "            \n",
    "            # Recorrer la lista de archivos y directorios\n",
    "            for item in contenido:\n",
    "                item_ruta = os.path.join(ruta_carpeta, item)\n",
    "                \n",
    "                # Si es un archivo, leer si es JSON\n",
    "                if os.path.isfile(item_ruta) and item.endswith('.json'):\n",
    "                    print(\"Leyendo el archivo:\", item_ruta)\n",
    "                    \n",
    "                    # Leer y procesar el archivo JSON\n",
    "                    with open(item_ruta, 'r', encoding='utf-8') as f:\n",
    "                        for line in f:\n",
    "                            df.append(json.loads(line))\n",
    "                \n",
    "                # Si es un directorio, llamar recursivamente a la función\n",
    "                elif os.path.isdir(item_ruta):\n",
    "                    print(f\"Entrando en carpeta: {item_ruta}\")\n",
    "                    subcontenido = leer_contenido_carpeta(item_ruta)\n",
    "                    if subcontenido is not None:\n",
    "                        df.extend(subcontenido)\n",
    "            \n",
    "            # Convertir la lista de diccionarios a DataFrame\n",
    "            df_final = pd.DataFrame(df)\n",
    "            \n",
    "            # Especificar la ruta donde se guardarán los archivos CSV\n",
    "            ruta_guardado = '../csv/Google Maps/reviews-estados'\n",
    "            \n",
    "            # Obtener el nombre de la carpeta que contiene los archivos JSON\n",
    "            nombre_carpeta = os.path.basename(ruta_carpeta)\n",
    "            \n",
    "            # Guardar el DataFrame como CSV en la ruta especificada\n",
    "            nombre_archivo_final = os.path.join(ruta_guardado, nombre_carpeta + '.csv')\n",
    "            df_final.to_csv(nombre_archivo_final, index=False)\n",
    "            print(\"Se guardó con éxito en:\", nombre_archivo_final)                    \n",
    "                    \n",
    "        else:\n",
    "            print(f\"La ruta '{ruta_carpeta}' no es un directorio válido.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el contenido de la carpeta: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leer_contenido_carpeta('../Data/Google Maps/reviews-estados/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def leer_contenido_carpeta(ruta_carpeta):\n",
    "    \"\"\" Lee el contenido de una carpeta y devuelve una lista de archivos y directorios.\n",
    "\n",
    "    Args:\n",
    "        ruta_carpeta (str): Ruta de la carpeta a leer.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de archivos y directorios.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Verificar si la ruta es un directorio\n",
    "        if os.path.isdir(ruta_carpeta):\n",
    "            # Obtener lista de archivos y directorios en la carpeta\n",
    "            contenido = os.listdir(ruta_carpeta)\n",
    "            \n",
    "            # Lista para almacenar datos de todos los archivos JSON en la carpeta\n",
    "            df = []\n",
    "            \n",
    "            # Recorrer la lista de archivos y directorios\n",
    "            for item in contenido:\n",
    "                item_ruta = os.path.join(ruta_carpeta, item)\n",
    "                \n",
    "                # Si es un archivo, leer si es JSON\n",
    "                if os.path.isfile(item_ruta) and item.endswith('.json'):\n",
    "                    print(\"Leyendo el archivo:\", item_ruta)\n",
    "                    \n",
    "                    # Leer y procesar el archivo JSON\n",
    "                    with open(item_ruta, 'r', encoding='utf-8') as f:\n",
    "                        for line in f:\n",
    "                            df.append(json.loads(line))\n",
    "            \n",
    "            # Convertir la lista de diccionarios a DataFrame\n",
    "            df_final = pd.DataFrame(df)\n",
    "            \n",
    "            # Especificar la ruta donde se guardarán los archivos CSV\n",
    "            ruta_guardado = '../csv/Google Maps/metadata-sitios'\n",
    "            \n",
    "            # Obtener el nombre de la carpeta que contiene los archivos JSON\n",
    "            nombre_carpeta = os.path.basename(ruta_carpeta)\n",
    "            \n",
    "            # Guardar el DataFrame como CSV en la ruta especificada\n",
    "            nombre_archivo_final = os.path.join(ruta_guardado, nombre_carpeta + '.csv')\n",
    "            df_final.to_csv(nombre_archivo_final, index=False)\n",
    "            print(\"Se guardó con éxito en:\", nombre_archivo_final)                    \n",
    "                    \n",
    "        else:\n",
    "            print(f\"La ruta '{ruta_carpeta}' no es un directorio válido.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el contenido de la carpeta: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leer_contenido_carpeta('../Data/Google Maps/metadata-sitios/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp User Parquet dividido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_archivo_parquet_en_csv(ruta_archivo_parquet, carpeta_destino_csv, num_filas_por_archivo=450000):\n",
    "    \"\"\" Divide un archivo Parquet en partes más pequeñas y las guarda como CSV.\n",
    "    El archivo Parquet se lee y se divide en partes más pequeñas. Cada parte se guarda como un archivo CSV.\n",
    "    El número de filas por archivo se puede establecer como un parámetro opcional.\n",
    "    Por defecto, el número de filas por archivo es de 450000.\n",
    "    El archivo CSV se guarda en la carpeta especificada como parámetro.\n",
    "    Si la carpeta no existe, se crea.\n",
    "    Si el archivo Parquet no existe, se levanta una excepción.\n",
    "    Si el archivo Parquet está vacío, se levanta una excepción.\n",
    "    Si el archivo Parquet tiene menos filas que el número de filas por archivo, se levanta una excepción.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Leer el archivo Parquet\n",
    "    archivo_grande = pd.read_parquet(ruta_archivo_parquet)\n",
    "\n",
    "    # Dividir el archivo en partes más pequeñas\n",
    "    num_archivos = (len(archivo_grande) // num_filas_por_archivo) + 1\n",
    "\n",
    "    for i in range(num_archivos):\n",
    "        inicio = i * num_filas_por_archivo\n",
    "        fin = min((i + 1) * num_filas_por_archivo, len(archivo_grande))  \n",
    "        archivo_pequeno = archivo_grande.iloc[inicio:fin]\n",
    "        \n",
    "        # Guardar el archivo como CSV\n",
    "        nombre_archivo_csv = f'archivo_pequeno_{i}.csv'\n",
    "        ruta_completa_csv = os.path.join(carpeta_destino_csv, nombre_archivo_csv)\n",
    "        archivo_pequeno.to_csv(ruta_completa_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_grande = '../Data/Yelp/user.parquet'\n",
    "carpeta_archivos_pequenos_csv = '../csv/Yelp/user/'\n",
    "dividir_archivo_parquet_en_csv(ruta_archivo_grande, carpeta_archivos_pequenos_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_archivo_json(ruta_archivo_json, ruta_guardado):\n",
    "    \n",
    "    \"\"\" \n",
    "    Procesa un archivo JSON y lo guarda en un archivo CSV.\n",
    "    Args:\n",
    "        ruta_archivo_json (str): Ruta del archivo JSON a procesar.\n",
    "        ruta_guardado (str): Ruta donde se guardará el archivo CSV resultante.\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.isfile(ruta_archivo_json) and ruta_archivo_json.endswith('.json'):\n",
    "            nombre_archivo = os.path.splitext(os.path.basename(ruta_archivo_json))[0]\n",
    "            \n",
    "            # Verificar si el nombre del archivo es diferente de \"review\"\n",
    "            if nombre_archivo != \"review\":\n",
    "                print(\"Leyendo el archivo:\", ruta_archivo_json)\n",
    "                with open(ruta_archivo_json, 'r', encoding='utf-8') as f:\n",
    "                    data = [json.loads(line) for line in f]\n",
    "\n",
    "                # Convertir la lista de diccionarios a DataFrame\n",
    "                df = pd.DataFrame(data)\n",
    "\n",
    "                # Guardar el DataFrame como CSV en la ruta especificada\n",
    "                nombre_archivo_csv = os.path.join(ruta_guardado, nombre_archivo + '.csv')\n",
    "                df.to_csv(nombre_archivo_csv, index=False)\n",
    "                print(\"Se guardó con éxito en:\", nombre_archivo_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el archivo JSON: {e}\")\n",
    "\n",
    "def leer_contenido_carpeta(ruta_carpeta, ruta_guardado):\n",
    "    try:\n",
    "        # Verificar si la ruta es un directorio\n",
    "        if os.path.isdir(ruta_carpeta):\n",
    "            # Obtener lista de archivos y directorios en la carpeta\n",
    "            contenido = os.listdir(ruta_carpeta)\n",
    "            \n",
    "            # Recorrer la lista de archivos y directorios\n",
    "            for item in contenido:\n",
    "                item_ruta = os.path.join(ruta_carpeta, item)\n",
    "                \n",
    "                # Si es un archivo, procesar si es JSON\n",
    "                if os.path.isfile(item_ruta):\n",
    "                    procesar_archivo_json(item_ruta, ruta_guardado)\n",
    "                \n",
    "                # Si es un directorio, llamar recursivamente a la función\n",
    "                elif os.path.isdir(item_ruta):\n",
    "                    print(f\"Entrando en carpeta: {item_ruta}\")\n",
    "                    leer_contenido_carpeta(item_ruta, ruta_guardado)\n",
    "        else:\n",
    "            print(f\"La ruta '{ruta_carpeta}' no es un directorio válido.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el contenido de la carpeta: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_carpeta = '../Data/Yelp/'\n",
    "ruta_guardado = '../csv/Yelp/'  \n",
    "leer_contenido_carpeta(ruta_carpeta, ruta_guardado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp review devidido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_archivo_json_en_csv(ruta_json_grande, carpeta_archivos_pequenos, tamano_archivo_pequeno=450000):\n",
    "    \n",
    "    \"\"\" \n",
    "    Dividir un archivo JSON grande en archivos CSV pequeños.\n",
    "\n",
    "    Parámetros:\n",
    "        ruta_json_grande (str): Ruta del archivo JSON grande.\n",
    "        carpeta_archivos_pequenos (str): Ruta de la carpeta donde se guardarán los archivos CSV pequeños.\n",
    "        tamano_archivo_pequeno (int): Tamaño máximo de cada archivo CSV pequeño (en líneas). Por defecto, 450000.\n",
    "\n",
    "    Retorna:\n",
    "        None. Los archivos CSV pequeños se guardan en la carpeta especificada.\n",
    "    \"\"\"\n",
    "    # Función para escribir un fragmento JSON en un archivo CSV\n",
    "    def escribir_fragmento_csv(numero_archivo, fragmento):\n",
    "        nombre_archivo = f'fragmento_{numero_archivo}.csv'\n",
    "        ruta_archivo = os.path.join(carpeta_archivos_pequenos, nombre_archivo)\n",
    "        with open(ruta_archivo, 'w', newline='', encoding='utf-8') as archivo_csv:\n",
    "            escritor_csv = csv.DictWriter(archivo_csv, fieldnames=fragmento[0].keys())\n",
    "            escritor_csv.writeheader()\n",
    "            escritor_csv.writerows(fragmento)\n",
    "\n",
    "    # Leer el archivo JSON grande y dividirlo en fragmentos más pequeños\n",
    "    with open(ruta_json_grande, 'r', encoding='utf-8') as archivo_grande:\n",
    "        fragmento_actual = []\n",
    "        numero_archivo = 1\n",
    "        for linea in archivo_grande:\n",
    "            objeto_json = json.loads(linea)\n",
    "            fragmento_actual.append(objeto_json)\n",
    "            if len(fragmento_actual) == tamano_archivo_pequeno:\n",
    "                escribir_fragmento_csv(numero_archivo, fragmento_actual)\n",
    "                numero_archivo += 1\n",
    "                fragmento_actual = []\n",
    "        # Escribir el último fragmento si es necesario\n",
    "        if fragmento_actual:\n",
    "            escribir_fragmento_csv(numero_archivo, fragmento_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_json_grande = '../Data/Yelp/review.json'\n",
    "carpeta_archivos_pequenos = '../csv/Yelp/review/'\n",
    "dividir_archivo_json_en_csv(ruta_json_grande, carpeta_archivos_pequenos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee el archivo pickle desde la ubicación especificada\n",
    "buis = pd.read_pickle('../Data/Yelp/business.pkl')\n",
    "\n",
    "# Guarda el DataFrame como un archivo CSV en la ubicación deseada\n",
    "ruta_guardado = '../csv/Yelp/business.csv'\n",
    "buis.to_csv(ruta_guardado, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajando con CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan las comunas pics y resp ya que están prácticamente vacías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_csv_estados(ruta_carpeta):\n",
    "    \"\"\" Función para limpiar los archivos CSV de la carpeta de estados de USA.\n",
    "    Elimina las columnas \"pics\" y \"resp\" del archivo CSV.\n",
    "    Guarda el archivo CSV en la carpeta \"Data_limpia/Google_Maps/review_estados\" con el mismo nombre.\n",
    "    Imprime la cantidad de Nulos por columna del archivo CSV.\n",
    "    Llama a la función recursiva para limpiar los directorios contenidos en la carpeta.\n",
    "    Recibe la ruta de la carpeta como parámetro.\n",
    "    \n",
    "    Args: ruta_carpeta (str): Ruta de la carpeta que contiene los archivos csv.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Verificar si la carpeta existe\n",
    "        if os.path.isdir(ruta_carpeta):\n",
    "            # Obtener lista de archivos y directorios en la carpeta\n",
    "            archivos = os.listdir(ruta_carpeta)\n",
    "            \n",
    "            # Recorrer la lista de archivos y directorios\n",
    "            for archivo in archivos:\n",
    "                # Verificar si el archivo es un directorio\n",
    "                if os.path.isdir(os.path.join(ruta_carpeta, archivo)):\n",
    "                    # Llamar a la función recursiva para limpiar el directorio\n",
    "                    limpiar_csv_estados(os.path.join(ruta_carpeta, archivo))\n",
    "                else:\n",
    "                    # Verificar si el archivo es un CSV\n",
    "                    if archivo.endswith('.csv'):\n",
    "                        #Abrir csv y eliminar columnas innecesarias\n",
    "                        print(f'Entrando al archivo : {archivo}')\n",
    "                        df = pd.read_csv(os.path.join(ruta_carpeta, archivo))\n",
    "                        df.drop(columns=['pics','resp'], inplace=True)\n",
    "                        #Guardar csv\n",
    "                        ruta_guardado = '../Data_limpia/Google_Maps/review_estados/'\n",
    "                        df.to_csv(os.path.join(ruta_guardado, archivo), index=False)\n",
    "                        print(f'Se eliminaron las columnas \"pics\" y \"resp\" del archivo {archivo} y se guardó con éxito en: {os.path.join(ruta_guardado, archivo)}')\n",
    "                        print(f'La cantidad de Nulos por columna del archivo {archivo} es la siguiente:\\n{df.isnull().sum()}')\n",
    "                    else:\n",
    "                        print(f'El archivo {archivo} no es un CSV, se pudo leer')\n",
    "        else:\n",
    "            print(f\"La carpeta {ruta_carpeta} no existe\") \n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el contenido de la carpeta: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrando al archivo : review-Arizona.csv\n",
      "Se eliminaron las columnas \"pics\" y \"resp\" del archivo review-Arizona.csv y se guardó con éxito en: ../Data_limpia/Google_Maps/review_estados/review-Arizona.csv\n",
      "La cantidad de Nulos por columna del archivo review-Arizona.csv es la siguiente:\n",
      "user_id         0\n",
      "name            7\n",
      "time            0\n",
      "rating          0\n",
      "text       832205\n",
      "gmap_id         0\n",
      "dtype: int64\n",
      "Entrando al archivo : review-California.csv\n",
      "Se eliminaron las columnas \"pics\" y \"resp\" del archivo review-California.csv y se guardó con éxito en: ../Data_limpia/Google_Maps/review_estados/review-California.csv\n",
      "La cantidad de Nulos por columna del archivo review-California.csv es la siguiente:\n",
      "user_id          0\n",
      "name             7\n",
      "time             0\n",
      "rating           0\n",
      "text       1170997\n",
      "gmap_id          0\n",
      "dtype: int64\n",
      "Entrando al archivo : review-Nevada.csv\n",
      "Se eliminaron las columnas \"pics\" y \"resp\" del archivo review-Nevada.csv y se guardó con éxito en: ../Data_limpia/Google_Maps/review_estados/review-Nevada.csv\n",
      "La cantidad de Nulos por columna del archivo review-Nevada.csv es la siguiente:\n",
      "user_id         0\n",
      "name            0\n",
      "time            0\n",
      "rating          0\n",
      "text       735172\n",
      "gmap_id         0\n",
      "dtype: int64\n",
      "Entrando al archivo : review-Oregon.csv\n",
      "Se eliminaron las columnas \"pics\" y \"resp\" del archivo review-Oregon.csv y se guardó con éxito en: ../Data_limpia/Google_Maps/review_estados/review-Oregon.csv\n",
      "La cantidad de Nulos por columna del archivo review-Oregon.csv es la siguiente:\n",
      "user_id         0\n",
      "name           37\n",
      "time            0\n",
      "rating          0\n",
      "text       897141\n",
      "gmap_id         0\n",
      "dtype: int64\n",
      "Entrando al archivo : review-Washington.csv\n",
      "Se eliminaron las columnas \"pics\" y \"resp\" del archivo review-Washington.csv y se guardó con éxito en: ../Data_limpia/Google_Maps/review_estados/review-Washington.csv\n",
      "La cantidad de Nulos por columna del archivo review-Washington.csv es la siguiente:\n",
      "user_id         0\n",
      "name           64\n",
      "time            0\n",
      "rating          0\n",
      "text       794112\n",
      "gmap_id         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "limpiar_csv_estados('../csv/Google Maps/costa_oeste/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata y Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerar\\AppData\\Local\\Temp\\ipykernel_12524\\718445187.py:3: DtypeWarning: Columns (14,15,16,17,18,19,25,26,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_business = pd.read_csv('../csv/Yelp/business.csv')\n"
     ]
    }
   ],
   "source": [
    "# Se extraen datos\n",
    "df_metatada = pd.read_csv('../csv/Google Maps/metadata-sitios/metadata-sitios.csv')\n",
    "df_business = pd.read_csv('../csv/Yelp/business.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea la columna coordenadas y se elimina latitud y longitud para un manejo más conveniente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metatada['coordenadas'] = df_metatada.apply(lambda row: (row['latitude'], row['longitude']), axis=1)\n",
    "df_metatada.drop(columns=['latitude', 'longitude'], inplace=True)\n",
    "df_metatada['coordenadas'] = df_metatada['coordenadas'].apply(lambda x: ', '.join(map(str,x)))\n",
    "\n",
    "df_business['coordenadas'] = df_business.apply(lambda row: (row['latitude'], row['longitude']), axis=1)\n",
    "df_business.drop(columns=['latitude', 'longitude'], inplace=True)\n",
    "df_business['coordenadas'] = df_business['coordenadas'].apply(lambda x: ', '.join(map(str,x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se abre el contenido de la columna \"category\" y se tratan los nulos de esta columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar NaN con listas vacías\n",
    "df_metatada['category'] = df_metatada['category'].fillna('[]')\n",
    "\n",
    "# Convertir las cadenas de caracteres en listas de Python\n",
    "df_metatada['category'] = df_metatada['category'].apply(ast.literal_eval)\n",
    "\n",
    "# Aplicar explode\n",
    "df_metatada = df_metatada.explode('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se extraen solo las columnas necesarias y se unen a través de la columna \"coordenadas\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metatada_v2 = df_metatada[['name','category','avg_rating','coordenadas','num_of_reviews']]\n",
    "df_business_v2 = df_business[['business_id','city','coordenadas','state','stars','review_count']]\n",
    "\n",
    "df_buis_metadata = pd.merge(df_metatada_v2, df_business_v2, on='coordenadas', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como solo se va a trabajar con 5 estados, se seleccionan solo las filas que tengan alguno de estos estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BuMet_Costa_Oeste = df_buis_metadata[df_buis_metadata['state'].isin(['CA', 'OR','WA','AZ','NV'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guardan los datos en csv para su posterior uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BuMet_Costa_Oeste.to_csv('../Data_limpia/Google_Maps/Buismet_Costa_Oeste.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan las columnas useful, funny y cool por su falta de relevancia para el proyecto final, se extraen solo las filas que coincidan con los locales que se va a trabajar aclaras en el csv \"Buismet_Costa_Oeste\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def eleccion_review(ruta_carpeta):\n",
    "    \"\"\" Función para limpiar los archivos de reviews.\n",
    "    \n",
    "    Args: ruta_carpeta (str): Ruta de la carpeta que contiene los archivos csv.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # verifica si la carpeta existe\n",
    "        if os.path.isdir(ruta_carpeta):\n",
    "            # obtiene una lista de archivos en la carpeta\n",
    "            archivos = os.listdir(ruta_carpeta)\n",
    "            \n",
    "            # Lista para almacenar todos los DataFrames fusionados\n",
    "            lista_df = []\n",
    "            \n",
    "            # itera sobre la lista de archivos\n",
    "            for archivo in archivos:\n",
    "                # verifica si el archivo es un directorio\n",
    "                if os.path.isdir(os.path.join(ruta_carpeta, archivo)):\n",
    "                    # llama recursivamente a la función para limpiar el directorio\n",
    "                    eleccion_review(os.path.join(ruta_carpeta, archivo))\n",
    "                else:\n",
    "                    #verifica si el archivo es un csv\n",
    "                    if archivo.endswith('.csv'):\n",
    "                        # abre el csv y elimina las columnas innecesarias\n",
    "                        print(f'Leyendo el archivo {archivo}...')\n",
    "                        df = pd.read_csv(os.path.join(ruta_carpeta, archivo))\n",
    "                        df.drop(columns=['useful','funny','cool'], inplace=True)\n",
    "                        \n",
    "                        # Convierte la columna de date al formato de date y hora si no está en ese formato\n",
    "                        df['date'] = pd.to_datetime(df['date'])\n",
    "                        \n",
    "                        # Filtra las filas para incluir solo aquellas con fechas a partir de 2015\n",
    "                        df = df[df['date'].dt.year >= 2015]\n",
    "                        \n",
    "                        # Lee el archivo \"Buismet_Costa_Oeste\" y elimina las columnas innecesarias\n",
    "                        # compara con el archivo \"Buismet_Costa_Oeste\" y solo se guardan las filas que coincidan ambos con business_id\n",
    "                        meta = pd.read_csv('../Data_limpia/Google_Maps/Buismet_Costa_Oeste.csv')\n",
    "                        df_final = pd.merge(df, meta[['business_id']], on='business_id', how='inner')\n",
    "                        # Agrega el DataFrame fusionado a la lista\n",
    "                        lista_df.append(df_final)\n",
    "                        print(f'Se eliminaron las columnas \\\"useful\\\", \\\"funny\\\", \\\"cool\\\" del archivo {archivo} y solo se extrajo las filas del año 2015 en adelante')\n",
    "                    else:\n",
    "                        print(f'El archivo {archivo} no es un CSV, se pudo leer')\n",
    "                        \n",
    "            # Fusiona todos los DataFrames en uno solo\n",
    "            df_final_total = pd.concat(lista_df, ignore_index=True)\n",
    "            \n",
    "            # Guarda el DataFrame fusionado en un solo archivo CSV\n",
    "            ruta_guardado = '../Data_limpia/Yelp/reviews/'\n",
    "            df_final_total.to_csv(os.path.join(ruta_guardado, 'reviews_desde_2015.csv'), index=False)\n",
    "            print(f'Se guardaron todos los archivos fusionados en: {os.path.join(ruta_guardado, \"reviews_desde_2015.csv\")}')\n",
    "            \n",
    "        else:\n",
    "            print(f\"La carpeta {ruta_carpeta} no existe\")   \n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el contenido de la carpeta: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo el archivo fragmento_1.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_1.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_10.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_10.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_11.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_11.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_12.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_12.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_13.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_13.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_14.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_14.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_15.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_15.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_16.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_16.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_2.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_2.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_3.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_3.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_4.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_4.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_5.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_5.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_6.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_6.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_7.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_7.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_8.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_8.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Leyendo el archivo fragmento_9.csv...\n",
      "Se eliminaron las columnas \"useful\", \"funny\", \"cool\" del archivo fragmento_9.csv y solo se extrajo las filas del año 2015 en adelante\n",
      "Se guardaron todos los archivos fusionados en: ../Data_limpia/Yelp/reviews/reviews_desde_2015.csv\n"
     ]
    }
   ],
   "source": [
    "eleccion_review('../csv/Yelp/review/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan las columnas ('review_count', 'yelping_since', 'useful', 'funny', 'cool', 'elite', 'friends', 'fans','average_stars', 'compliment_hot', 'compliment_more', 'compliment_profile', 'compliment_cute', 'compliment_list', 'compliment_note', 'compliment_plain', 'compliment_cool', 'compliment_funny', 'compliment_writer' y 'compliment_photos') por su falta de relevancia para el proyecto final, se extraen solo las filas que coincidan con los \"user_id\" que se va a trabajar aclaras en el csv \"reviews_desde_2015.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eleccion_users(ruta_carpeta):\n",
    "    \n",
    "    \"\"\" \n",
    "    Función que se encarga de limpiar los archivos csv de la carpeta \"users\"\n",
    "    y fusionarlos con el archivo \"reviews_desde_2015.csv\" para obtener un\n",
    "    archivo limpio de usuarios.\n",
    "    \n",
    "    Args: ruta_carpeta (str): Ruta de la carpeta que contiene los archivos csv.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # verifica si la carpeta existe\n",
    "        if os.path.isdir(ruta_carpeta):\n",
    "            # obtiene una lista de archivos en la carpeta\n",
    "            archivos = os.listdir(ruta_carpeta)\n",
    "            \n",
    "            # Lista para almacenar todos los DataFrames fusionados\n",
    "            lista_df = []\n",
    "            \n",
    "            # itera sobre la lista de archivos\n",
    "            for archivo in archivos:\n",
    "                # verifica si el archivo es un directorio\n",
    "                if os.path.isdir(os.path.join(ruta_carpeta, archivo)):\n",
    "                    # llama recursivamente a la función para limpiar el directorio\n",
    "                    eleccion_users(os.path.join(ruta_carpeta, archivo))\n",
    "                else:\n",
    "                    #verifica si el archivo es un csv\n",
    "                    if archivo.endswith('.csv'):\n",
    "                        # abre el csv y elimina las columnas innecesarias\n",
    "                        print(f'Leyendo el archivo {archivo}...')\n",
    "                        df = pd.read_csv(os.path.join(ruta_carpeta, archivo))\n",
    "                        \n",
    "                        print('Eliminando columnas innecesarias...')\n",
    "                        df.drop(columns=['review_count','yelping_since','useful','funny','cool','elite','friends','fans','average_stars','compliment_hot','compliment_more','compliment_profile','compliment_cute','compliment_list','compliment_note','compliment_plain','compliment_cool','compliment_funny','compliment_writer','compliment_photos'], inplace=True)\n",
    "                        # compara con el archivo \"reviews_desde_2015.csv\" y solo se guardan las filas que coincidan ambos con user_id\n",
    "                        rev = pd.read_csv('../Data_limpia/Yelp/reviews/reviews_desde_2015.csv')\n",
    "                        df_final = pd.merge(df, rev[['user_id']], on='user_id', how='inner')\n",
    "                        \n",
    "                        # Agrega el DataFrame fusionado a la lista\n",
    "                        lista_df.append(df_final)\n",
    "                        \n",
    "                    else:\n",
    "                        print(f'El archivo {archivo} no es un CSV, se pudo leer')\n",
    "                        \n",
    "            # Fusiona todos los DataFrames en uno solo\n",
    "            df_final_total = pd.concat(lista_df, ignore_index=True)\n",
    "            \n",
    "            # Guarda el DataFrame fusionado en un solo archivo CSV\n",
    "            ruta_guardado = '../Data_limpia/Yelp/users/'\n",
    "            df_final_total.to_csv(os.path.join(ruta_guardado, 'users_desde_2015.csv'), index=False)\n",
    "            print(f'Se guardaron todos los archivos fusionados en: {os.path.join(ruta_guardado, \"users_fusionados.csv\")}')\n",
    "            \n",
    "        else:\n",
    "            print(f\"La carpeta {ruta_carpeta} no existe\")   \n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el contenido de la carpeta: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo el archivo archivo_pequeno_0.csv...\n",
      "Eliminando columnas innecesarias...\n",
      "Leyendo el archivo archivo_pequeno_1.csv...\n",
      "Eliminando columnas innecesarias...\n",
      "Leyendo el archivo archivo_pequeno_2.csv...\n",
      "Eliminando columnas innecesarias...\n",
      "Leyendo el archivo archivo_pequeno_3.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gerar\\AppData\\Local\\Temp\\ipykernel_14204\\250418535.py:22: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(ruta_carpeta, archivo))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminando columnas innecesarias...\n",
      "Leyendo el archivo archivo_pequeno_4.csv...\n",
      "Eliminando columnas innecesarias...\n",
      "Se guardaron todos los archivos fusionados en: ../Data_limpia/Yelp/users/users_fusionados.csv\n"
     ]
    }
   ],
   "source": [
    "eleccion_users('../csv/Yelp/user/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('../Data_limpia/Yelp/reviews/reviews_desde_2015.csv')\n",
    "users = pd.read_csv('../Data_limpia/Yelp/users/users_desde_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_users_desde_2015 = pd.merge(users, reviews, on='user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IpLRJY4CP3fXtlEd8Y4GFQ</td>\n",
       "      <td>Robyn</td>\n",
       "      <td>rqpFNOI5LIYKI484x4SHpA</td>\n",
       "      <td>_vi0nhfx0jSlIOp8PzD8QQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Heard about this place via NBC-10 and the Elle...</td>\n",
       "      <td>2015-01-14 11:26:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IpLRJY4CP3fXtlEd8Y4GFQ</td>\n",
       "      <td>Robyn</td>\n",
       "      <td>rqpFNOI5LIYKI484x4SHpA</td>\n",
       "      <td>_vi0nhfx0jSlIOp8PzD8QQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Heard about this place via NBC-10 and the Elle...</td>\n",
       "      <td>2015-01-14 11:26:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IpLRJY4CP3fXtlEd8Y4GFQ</td>\n",
       "      <td>Robyn</td>\n",
       "      <td>rqpFNOI5LIYKI484x4SHpA</td>\n",
       "      <td>_vi0nhfx0jSlIOp8PzD8QQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Heard about this place via NBC-10 and the Elle...</td>\n",
       "      <td>2015-01-14 11:26:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IpLRJY4CP3fXtlEd8Y4GFQ</td>\n",
       "      <td>Robyn</td>\n",
       "      <td>sMqbUjo9g3DJVSQGa8tnpw</td>\n",
       "      <td>kI_8RT6-IoVxD-EgXmyFvg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The food was good enough, but they weren't ver...</td>\n",
       "      <td>2016-06-17 16:01:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IpLRJY4CP3fXtlEd8Y4GFQ</td>\n",
       "      <td>Robyn</td>\n",
       "      <td>sMqbUjo9g3DJVSQGa8tnpw</td>\n",
       "      <td>kI_8RT6-IoVxD-EgXmyFvg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The food was good enough, but they weren't ver...</td>\n",
       "      <td>2016-06-17 16:01:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563866</th>\n",
       "      <td>7iUcYua1GtbIcHjTqIiG3Q</td>\n",
       "      <td>Linda</td>\n",
       "      <td>R69rsGF2BchLAa7a7s_KMg</td>\n",
       "      <td>xmgLT96LecWFLQBaX-mB1w</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Truckee River Rock developed our landscaping p...</td>\n",
       "      <td>2015-12-29 18:50:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563867</th>\n",
       "      <td>f9Lm5TrNfVX1JmVXIbtCOg</td>\n",
       "      <td>Bernae</td>\n",
       "      <td>tUZZoZuDUF-RKBLALwC-qA</td>\n",
       "      <td>o4MAhdTefZhXHgDvx1pSvA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great location and the staff are friendly and ...</td>\n",
       "      <td>2016-06-03 23:41:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563868</th>\n",
       "      <td>f9Lm5TrNfVX1JmVXIbtCOg</td>\n",
       "      <td>Bernae</td>\n",
       "      <td>tUZZoZuDUF-RKBLALwC-qA</td>\n",
       "      <td>o4MAhdTefZhXHgDvx1pSvA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great location and the staff are friendly and ...</td>\n",
       "      <td>2016-06-03 23:41:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563869</th>\n",
       "      <td>f9Lm5TrNfVX1JmVXIbtCOg</td>\n",
       "      <td>Bernae</td>\n",
       "      <td>tUZZoZuDUF-RKBLALwC-qA</td>\n",
       "      <td>o4MAhdTefZhXHgDvx1pSvA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great location and the staff are friendly and ...</td>\n",
       "      <td>2016-06-03 23:41:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563870</th>\n",
       "      <td>f9Lm5TrNfVX1JmVXIbtCOg</td>\n",
       "      <td>Bernae</td>\n",
       "      <td>tUZZoZuDUF-RKBLALwC-qA</td>\n",
       "      <td>o4MAhdTefZhXHgDvx1pSvA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great location and the staff are friendly and ...</td>\n",
       "      <td>2016-06-03 23:41:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2563871 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id    name               review_id  \\\n",
       "0        IpLRJY4CP3fXtlEd8Y4GFQ   Robyn  rqpFNOI5LIYKI484x4SHpA   \n",
       "1        IpLRJY4CP3fXtlEd8Y4GFQ   Robyn  rqpFNOI5LIYKI484x4SHpA   \n",
       "2        IpLRJY4CP3fXtlEd8Y4GFQ   Robyn  rqpFNOI5LIYKI484x4SHpA   \n",
       "3        IpLRJY4CP3fXtlEd8Y4GFQ   Robyn  sMqbUjo9g3DJVSQGa8tnpw   \n",
       "4        IpLRJY4CP3fXtlEd8Y4GFQ   Robyn  sMqbUjo9g3DJVSQGa8tnpw   \n",
       "...                         ...     ...                     ...   \n",
       "2563866  7iUcYua1GtbIcHjTqIiG3Q   Linda  R69rsGF2BchLAa7a7s_KMg   \n",
       "2563867  f9Lm5TrNfVX1JmVXIbtCOg  Bernae  tUZZoZuDUF-RKBLALwC-qA   \n",
       "2563868  f9Lm5TrNfVX1JmVXIbtCOg  Bernae  tUZZoZuDUF-RKBLALwC-qA   \n",
       "2563869  f9Lm5TrNfVX1JmVXIbtCOg  Bernae  tUZZoZuDUF-RKBLALwC-qA   \n",
       "2563870  f9Lm5TrNfVX1JmVXIbtCOg  Bernae  tUZZoZuDUF-RKBLALwC-qA   \n",
       "\n",
       "                    business_id  stars  \\\n",
       "0        _vi0nhfx0jSlIOp8PzD8QQ    5.0   \n",
       "1        _vi0nhfx0jSlIOp8PzD8QQ    5.0   \n",
       "2        _vi0nhfx0jSlIOp8PzD8QQ    5.0   \n",
       "3        kI_8RT6-IoVxD-EgXmyFvg    2.0   \n",
       "4        kI_8RT6-IoVxD-EgXmyFvg    2.0   \n",
       "...                         ...    ...   \n",
       "2563866  xmgLT96LecWFLQBaX-mB1w    5.0   \n",
       "2563867  o4MAhdTefZhXHgDvx1pSvA    5.0   \n",
       "2563868  o4MAhdTefZhXHgDvx1pSvA    5.0   \n",
       "2563869  o4MAhdTefZhXHgDvx1pSvA    5.0   \n",
       "2563870  o4MAhdTefZhXHgDvx1pSvA    5.0   \n",
       "\n",
       "                                                      text  \\\n",
       "0        Heard about this place via NBC-10 and the Elle...   \n",
       "1        Heard about this place via NBC-10 and the Elle...   \n",
       "2        Heard about this place via NBC-10 and the Elle...   \n",
       "3        The food was good enough, but they weren't ver...   \n",
       "4        The food was good enough, but they weren't ver...   \n",
       "...                                                    ...   \n",
       "2563866  Truckee River Rock developed our landscaping p...   \n",
       "2563867  Great location and the staff are friendly and ...   \n",
       "2563868  Great location and the staff are friendly and ...   \n",
       "2563869  Great location and the staff are friendly and ...   \n",
       "2563870  Great location and the staff are friendly and ...   \n",
       "\n",
       "                        date  \n",
       "0        2015-01-14 11:26:56  \n",
       "1        2015-01-14 11:26:56  \n",
       "2        2015-01-14 11:26:56  \n",
       "3        2016-06-17 16:01:51  \n",
       "4        2016-06-17 16:01:51  \n",
       "...                      ...  \n",
       "2563866  2015-12-29 18:50:53  \n",
       "2563867  2016-06-03 23:41:01  \n",
       "2563868  2016-06-03 23:41:01  \n",
       "2563869  2016-06-03 23:41:01  \n",
       "2563870  2016-06-03 23:41:01  \n",
       "\n",
       "[2563871 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_users_desde_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filas_duplicadas = reviews_users_desde_2015.duplicated(keep=False)  # keep=False marca todas las filas duplicadas como True\n",
    "total_filas_duplicadas = filas_duplicadas.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2551382"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_filas_duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
